---
---

@misc{Leadbeater2021,
author = {Leadbeater, Chiara and Sharrock, Louis and Coyle, Brian and Benedetti, Marcello},
booktitle = {Entropy},
isbn = {1099-4300},
keywords = {born machine,f-divergence,generative modelling,local cost function},
number = {10},
title = {{F-divergences and cost function locality in generative modelling with quantum circuits}},
volume = {23},
year = {2021},
altmetric = {false},
abbr={Entropy},
abstract={Generative modelling is an important unsupervised task in machine learning. In this work, we study a hybrid quantum-classical approach to this task, based on the use of a quantum circuit born machine. In particular, we consider training a quantum circuit born machine using f-divergences. We first discuss the adversarial framework for generative modelling, which enables the estimation of any f-divergence in the near term. Based on this capability, we introduce two heuristics which demonstrably improve the training of the born machine. The first is based on f-divergence switching during training. The second introduces locality to the divergence, a strategy which has proved important in similar applications in terms of mitigating barren plateaus. Finally, we discuss the long-term implications of quantum devices for computing f-divergences, including algorithms which provide quadratic speedups to their estimation. In particular, we generalise existing algorithms for estimating the Kullback–Leibler divergence and the total variation distance to obtain a fault-tolerant quantum algorithm for estimating another f-divergence, namely, the Pearson divergence.},
arxiv={2110.04253},
html={https://www.mdpi.com/1099-4300/23/10/1281},
}
@article{Sharrock2020,
author = {Sharrock, Louis and Kantas, Nikolas},
journal = {SIAM /ASA Journal on Uncertainty Quantification},
number = {1},
pages = {55--95},
title = {{Joint online parameter estimation for the partially observed stochastic advection diffusion equation}},
volume = {10},
year = {2022},
abbr={SIAM},
altmetric = {false},
abstract = {In this paper, we consider the problem of jointly performing online parameter estimation and optimal sensor placement for a partially observed infinite-dimensional linear diffusion process. We present a novel solution to this problem in the form of a continuous-time, two-timescale stochastic gradient descent algorithm, which recursively seeks to maximize the asymptotic log-likelihood of the observations with respect to the unknown model parameters and to minimize the expected mean squared error of the hidden state estimate with respect to the sensor locations. We also provide extensive numerical results illustrating the performance of the proposed approach in the case that the hidden signal is governed by the two-dimensional stochastic advection-diffusion equation.},
arxiv={2009.08693},
html={https://epubs.siam.org/doi/10.1137/20M1375073},
}
@article{Sharrock2021a,
author = {Sharrock, Louis and Kantas, Nikolas and Parpas, Panos and Pavliotis, Grigorios A.},
journal = {Stochastic Processes and their Applications},
pages = {481--546},
title = {{Online Parameter Estimation for the McKean-Vlasov Stochastic Differential Equation}},
volume = {162},
year = {2023},
selected = {true},
abbr={SPA},
altmetric = {false},
abstract = {We analyse the problem of online parameter estimation for a stochastic McKean–Vlasov equation, and the associated system of weakly interacting particles. We propose an online estimator for the parameters of the McKean–Vlasov SDE, or the interacting particle system, which is based on a continuous-time stochastic gradient ascent scheme with respect to the asymptotic log-likelihood of the interacting particle system. We characterise the asymptotic behaviour of this estimator in the limit as $T\rightarrow\infty$, and also in the joint limit as  $T\rightarrow\infty$ and $N\rightarrow\infty$. In these two cases, we obtain almost sure or $L_1$ convergence to the stationary points of a limiting contrast function, under suitable conditions which guarantee ergodicity and uniform-in-time propagation of chaos. We also establish, under the additional condition of global strong concavity, $L_2$ convergence to the unique maximiser of the asymptotic log-likelihood of the McKean–Vlasov SDE, with an asymptotic convergence rate which depends on the learning rate, the number of observations, and the dimension of the non-linear process. Our theoretical results are supported by two numerical examples, a linear mean field model and a stochastic opinion dynamics model.},
arxiv={2106.13751},
html={https://www.sciencedirect.com/science/article/pii/S0304414923000972},
slides = {mvsde-slides.pdf},
code = {https://github.com/louissharrock/Parameter-Estimation-for-Interacting-Particle-Systems},
}

@inproceedings{Sharrock2022c,
author = {Sharrock, Louis},
booktitle = {ICML 2022 Workshop on Continuous Time Methods for Machine Learning},
title = {{Two-timescale stochastic approximation for bilevel optimisation problems in continuous-time models}},
year = {2022},
abbr={ICML},
altmetric = {false},
abstract = {We analyse the asymptotic properties of a continuous-time, two-timescale stochastic approximation algorithm designed for stochastic bilevel optimisation problems in continuous-time models. We obtain the weak convergence rate of this algorithm in the form of a central limit theorem. We also demonstrate how this algorithm can be applied to several continuous-time bilevel optimisation problems.},
arxiv={2206.06995},
}

@article{Sharrock2020a,
author = {Sharrock, Louis and Kantas, Nikolas},
journal = {Bernoulli},
number = {2},
pages = {1137--1165},
title = {{Two-timescale stochastic gradient descent in continuous time with applications to joint online parameter estimation and optimal sensor placement}},
volume = {29},
year = {2023},
selected = {true},
abbr={Bernoulli},
altmetric = {false},
abstract = {In this paper, we establish the almost sure convergence of two-timescale stochastic gradient descent algorithms in continuous time under general noise and stability conditions, extending well known results in discrete time. We analyse algorithms with additive noise and those with non-additive noise. In the non-additive case, our analysis is carried out under the assumption that the noise is a continuous-time Markov process, controlled by the algorithm states. The algorithms we consider can be applied to a broad class of bilevel optimisation problems. We study one such problem in detail, namely, the problem of joint online parameter estimation and optimal sensor placement for a partially observed diffusion process. We demonstrate how this can be formulated as a bilevel optimisation problem, and propose a solution in the form of a continuous-time, two-timescale, stochastic gradient descent algorithm. Furthermore, under suitable conditions on the latent signal, the filter, and the filter derivatives, we establish almost sure convergence of the online parameter estimates and optimal sensor placements to the stationary points of the asymptotic log-likelihood and asymptotic filter covariance, respectively. We also provide numerical examples, illustrating the application of the proposed methodology to a partially observed Beneš equation, and a partially observed stochastic advection-diffusion equation.}, 
arxiv={2007.15998},
html={https://projecteuclid.org/journals/bernoulli/volume-29/issue-2/Two-timescale-stochastic-gradient-descent-in-continuous-time-with-applications/10.3150/22-BEJ1493.full},
}

@article{Sharrock2022d,
author = {Sharrock, Louis and Simons, Jack and Liu, Song and Beaumont, Mark},
journal = {arXiv preprint},
title = {{Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models}},
year = {2022},
altmetric = {false},
abbr={arXiv},
abstract = {We introduce Sequential Neural Posterior Score Estimation (SNPSE) and Sequential Neural Likelihood Score Estimation (SNLSE), two new score-based methods for Bayesian inference in simulator-based models. Our methods, inspired by the success of score-based methods in generative modelling, leverage conditional score-based diffusion models to generate samples from the posterior distribution of interest. These models can be trained using one of two possible objective functions, one of which approximates the score of the intractable likelihood, while the other directly estimates the score of the posterior. We embed these models into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We validate our methods, as well as their amortised, non-sequential variants, on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE) and Sequential Neural Likelihood Estimation (SNLE).}, 
arxiv={2210.04872},
}

@article{Sharrock2023,
archivePrefix = {arXiv},
arxivId = {2301.11294},
author = {Sharrock, Louis and Nemeth, Christopher},
eprint = {2301.11294},
journal = {Proceedings of the 40th International Conference on Machine Learning (ICML 2023)},
title = {{Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates}},
year = {2023}, 
selected={true},
abbr={ICML},
altmetric = {false},
abstract = {In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.}, 
arxiv={2301.11294},
code = {https://github.com/louissharrock/Coin-SVGD},
slides = {coin_slides.pdf},
poster = {coin-poster.pdf},
}

@article{Sharrock2023a,
author = {Sharrock, Louis and Mackey, Lester and Nemeth, Christopher},
journal = {arXiv preprint},
journal = {To appear in Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)},
title = {{Learning Rate Free Bayesian Inference in Constrained Domains}},
year = {2023},
altmetric = {false},
abbr={NeurIPS},
abstract = {We introduce a suite of new particle-based algorithms for sampling on constrained domains which are entirely learning rate free. Our approach leverages coin betting ideas from convex optimisation, and the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures. Based on this viewpoint, we also introduce a unifying framework for several existing constrained sampling algorithms, including mirrored Langevin dynamics and mirrored Stein variational gradient descent. We demonstrate the performance of our algorithms on a range of numerical examples, including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. Our results indicate that our algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.},
arxiv={2305.14943},
}

@article{Sharrock2023b,
author = {Sharrock, Louis and Dodd, Daniel and Nemeth, Christopher},
journal = {arXiv preprint},
title = {{CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models}},
year = {2023},
altmetric = {false},
abbr={arXiv},
abstract = {We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the free energy which is entirely learning rate free, based on coin betting techniques from convex optimization. We validate the performance of our algorithms across a broad range of numerical experiments, including several high-dimensional settings. Our results are competitive with existing particle-based methods, without the need for any hyperparameter tuning.},
arxiv={2305.14916},
}

